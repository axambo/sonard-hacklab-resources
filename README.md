# AI Performance Playground - Sonar+D Hacklab Resources

Inspired by and adapted from the [Timbre Resources webpage](https://github.com/comma-lab/timbre-resources), this is a living document of resources to support the development of hacks & prototypes for the [AI Performance Playground organised by SÃ³nar+D 2025](https://sonar.es/en/programme/sonar-d/open-call-hacklab) with the goal of exploring and deepening the use of machine learning tools, AI, and other related technologies with a critical perspective. Collaboratively and through the exchange of skills and knowledge, the aim is to learn to critically apply new tools in musical and performative practices, and what surrounds them. 

## User-friendly systems

[Audiostellar](https://audiostellar.xyz) - AI-powered experimental sampler

## Plug-ins

[Concatenator](https://datamindaudio.ai/concatenator-v1/) (DataMind Audio) - AI-powered audio mosaicing plug-in.

## Machine learning and AI

[SP-Tools](https://rodrigoconstanzo.com/sp-tools/) | A set of machine learning tools that are optimised for low latency and real-time performance. The tools can be used with [Sensory Percussion sensors](http://sunhou.se/), ordinary drum triggers, or any audio input.

[FluCoMa](https://www.flucoma.org/) | The FluCoMa software consists of objects for decomposing and describing audio, and for manipulating collections of sonic data by querying, matching, learning and transforming. The complete toolkit is available for Max, SuperCollider and Pure Data, and the decomposition / description tools are available for the command line.

[nn~](https://acids-ircam.github.io/nn_tilde/) | At its core, nn~ is a translation layer between Max/MSP or PureData and the [libtorch c++ interface for deep learning](https://pytorch.org/). Alone, nn~ is like an empty shell, and requires pretrained models to operate. Using nn~ for PureData, RAVE can be used in realtime on embedded platforms, such as Bela or Raspberry Pi 4.

[neutone.space](https://neutone.space/) | A platform where researchers can share real-time AI audio processing models for creators to experiment with transformative AI audio instruments.

## Models

You can find a few [RAVE](https://github.com/acids-ircam/rave) models [here](https://acids-ircam.github.io/rave_models_download).

[Shuoyang Zheng/Jasper's RAVE models](https://huggingface.co/shuoyang-zheng/jaspers-rave-models).

You can find a few [vschaos2](https://github.com/acids-ircam/vschaos2) models [here](https://www.dropbox.com/sh/avdeiza7c6bn2of/AAAGZsnRo9ZVMa0iFhouCBL-a?dl=0).

## Datasets

[SOL](https://forum.ircam.fr/collections/detail/sol-instrumental-sounds-datasets/) | Ircam instrumental sound database coming from the Studio On Line project.

[Nsynth](https://magenta.tensorflow.org/datasets/nsynth) | 305,979 musical notes, each with a unique pitch, timbre, and envelope.

[Freesound](https://freesound.org/) | a collaborative collection of 618,244 free sounds

[OpenMIC-2018](https://zenodo.org/records/1432913#.W6dPeJNKjOR) | 20000 audio clips with annotations of the presence or absence of 20 instrument classes

[URMP](http://labsites.rochester.edu/air/projects/URMP.html) | 44 pieces of orchestral recordings with note-level and frame-level annotations

[MIS](https://theremin.music.uiowa.edu/MIS.html) | single instrument notes with different playing techniques

[Medley-solos-DB](https://zenodo.org/records/2582103) | an instrument recognition dataset, audio extracted from MedleyDB and solosDB

## Hardware

## Miscellaneous

## Readings and talks
