# AI Performance Playground - Sonar+D Hacklab Resources

Inspired by and adapted from the [Timbre Resources webpage](https://github.com/comma-lab/timbre-resources), this is a living document of resources to support the development of hacks & prototypes for the [AI Performance Playground organised by Sónar+D 2025](https://sonar.es/en/programme/sonar-d/open-call-hacklab) with the goal of exploring and deepening the use of machine learning tools, AI, and other related technologies with a critical perspective. Some of the tools have been mentioned by the Hacklab participants as part of their arsenal. The resources mainly focus on sound with a section related to visuals. Collaboratively and through the exchange of skills and knowledge, the aim of the Hacklab is to learn collectively to critically apply new tools in musical and performative practices, and what surrounds them. 

## User-friendly systems

[Audiostellar](https://audiostellar.xyz) - AI-powered experimental sampler

[Wekinator](http://www.wekinator.org) - Machine learning for building new musical instruments, gestural game controllers, computer vision or computer listening systems, among others.

[Stable Audio SPA](https://stableaudio.com) - a web app that generates 3 minute songs in 10 seconds.

## Plug-ins

[Concatenator](https://datamindaudio.ai/concatenator-v1/) (DataMind Audio) - AI-powered audio mosaicing plug-in.

## Machine learning and AI

[SP-Tools](https://rodrigoconstanzo.com/sp-tools/) | A set of machine learning tools that are optimised for low latency and real-time performance. The tools can be used with [Sensory Percussion sensors](http://sunhou.se/), ordinary drum triggers, or any audio input.

[FluCoMa](https://www.flucoma.org/) | The FluCoMa software consists of objects for decomposing and describing audio, and for manipulating collections of sonic data by querying, matching, learning and transforming. The complete toolkit is available for Max, SuperCollider and Pure Data, and the decomposition / description tools are available for the command line.

## Generative AI

[Stable Audio Open](https://stable-audio-open.com) - an open source model optimised for generating short audio samples, sound effects and production elements using text prompts.

## Neural sound synthesis

[nn~](https://acids-ircam.github.io/nn_tilde/) | At its core, nn~ is a translation layer between Max/MSP or PureData and the [libtorch c++ interface for deep learning](https://pytorch.org/). Alone, nn~ is like an empty shell, and requires pretrained models to operate. 

[neutone.space](https://neutone.space/) | A platform where researchers can share real-time AI audio processing models for creators to experiment with transformative AI audio instruments.

## Models

You can find a few [RAVE](https://github.com/acids-ircam/rave) models [here](https://acids-ircam.github.io/rave_models_download).

[Shuoyang Zheng/Jasper's RAVE models](https://huggingface.co/shuoyang-zheng/jaspers-rave-models).

You can find a few [vschaos2](https://github.com/acids-ircam/vschaos2) models [here](https://www.dropbox.com/sh/avdeiza7c6bn2of/AAAGZsnRo9ZVMa0iFhouCBL-a?dl=0).

Stable Audio Open model available on [huggingface](https://huggingface.co/stabilityai/stable-audio-open-1.0).

## Datasets

[SOL](https://forum.ircam.fr/collections/detail/sol-instrumental-sounds-datasets/) | Ircam instrumental sound database coming from the Studio On Line project.

[Nsynth](https://magenta.tensorflow.org/datasets/nsynth) | 305,979 musical notes, each with a unique pitch, timbre, and envelope.

[Freesound](https://freesound.org/) | a collaborative collection of 618,244 free sounds.

[OpenMIC-2018](https://zenodo.org/records/1432913#.W6dPeJNKjOR) | 20000 audio clips with annotations of the presence or absence of 20 instrument classes.

[URMP](http://labsites.rochester.edu/air/projects/URMP.html) | 44 pieces of orchestral recordings with note-level and frame-level annotations.

[MIS](https://theremin.music.uiowa.edu/MIS.html) | single instrument notes with different playing techniques.

[Medley-solos-DB](https://zenodo.org/records/2582103) | an instrument recognition dataset, audio extracted from MedleyDB and solosDB.

## Hardware

Using nn~ for PureData, RAVE can be used in realtime on embedded platforms, such as Bela or Raspberry Pi 4.

## Other software, platforms, resources (miscellaneous)

[Hugging Face](https://huggingface.co) - a resourceful collaborative platform with models, datasets, and applications.

[Cursor](https://www.cursor.com) - AI code editor.

## Visual (miscellaneous)

[Stable Diffusion](https://github.com/Stability-AI/generative-models) - a deep learning, text-to-image model based on diffusion techniques.

[Figma AI](https://www.figma.com/ai/) - a visualisation tool for idea representation.

[ComfyUI](https://www.comfy.org) - generate video, images, 3D, audio with AI.

## Readings and talks

* [Book] Nao Tokui (2023) ["Surfing human creativity with AI — A user’s guide"](https://naotokui.net/works/surfing-human-creativity-with-ai-a-users-guide/)

* [Talk] Lauren Klein (April 24, 2024) ["Data Feminism for AI"](https://youtu.be/p9vbNA5_t44) (1:16:07)

* [Paper] Lauren Klein, Catherine D'Ignazio (2024) ["Data Feminism for AI"](https://facctconference.org/static/papers24/facct24-7.pdf) (FAccT ’24, June 03–06, 2024, Rio de Janeiro, Brazil)

## Performances

